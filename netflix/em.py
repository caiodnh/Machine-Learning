"""Mixture model for matrix completion"""
# import pdb; pdb.set_trace()
from typing import Tuple
import numpy as np
from numpy.lib.scimath import log
from scipy.special import logsumexp
from scipy.stats import multivariate_normal
from common import GaussianMixture


def estep(X: np.ndarray, mixture: GaussianMixture) -> Tuple[np.ndarray, float]:
    """E-step: Softly assigns each datapoint to a gaussian component

    Args:
        X: (n, d) array holding the data, with incomplete entries (set to 0)
        mixture: the current gaussian mixture

    Returns:
        np.ndarray: (n, K) array holding the soft counts
            for all components for all examples
        float: log-likelihood of the assignment

    """
    n = X.shape[0]
    K = mixture.p.shape[0]

    f = np.empty((n,K))
    # the entry (i,j) is the value
    # f(i, j) = log (prob of cluster j) + log(prob density of point X[i] being generated by cluster j)
    for (i,x) in enumerate(X):
        # find the support of point x and the clean_x
        supp = x != 0
        dim = supp.sum()
        clean_x = x[supp]

        # Compute the pdf for each cluster
        # clean_mus = mixture.mu[:,supp] # K rows, each one is the mean of a cluster
        # diff = clean_x - clean_mus # K rows, each one is x - mu
        # normSq = np.sum(diff**2, axis = 1) # K rows, each one is the sum of squares
        # var = mixture.var # K rows, each one is the variance
        # logpdf = - normSq / (2 * var) - dim * np.log(2 * np.pi * var) / 2 # Calculation in each row
        # f[i] = logpdf

        for j in range(K):
            mu = mixture.mu[j][supp]
            var = mixture.var[j]
            logpdf = - np.sum((clean_x - mu)**2)/ (2*var) - dim * np.log(2*np.pi*var) / 2
            f[i, j] = logpdf

    f = f + np.log(mixture.p + 1e-16)

    point_log_likelihood = np.apply_along_axis(logsumexp, 1 , f)

    log_posterior = f - point_log_likelihood[:, None]

    soft_counts = np.exp(log_posterior)
    log_likelihood = np.sum(point_log_likelihood)

    return soft_counts, log_likelihood


def mstep(X: np.ndarray, post: np.ndarray, mixture: GaussianMixture,
          min_variance: float = .25) -> GaussianMixture:
    """M-step: Updates the gaussian mixture by maximizing the log-likelihood
    of the weighted dataset

    Args:
        X: (n, d) array holding the data, with incomplete entries (set to 0)
        post: (n, K) array holding the soft counts
            for all components for all examples
        mixture: the current gaussian mixture
        min_variance: the minimum variance for each gaussian

    Returns:
        GaussianMixture: the new gaussian mixture
    """
    delta = X != 0 # shape (n, d)

    count_soft_points = post.T @ delta # (K, d)
    mus = post.T @ (delta * X) / count_soft_points # (K, d)

    # correct the entries with less than 1 soft point
    for (i, nums) in enumerate(count_soft_points):
        for (j, num) in enumerate(nums):
            if num < 1:
                mus[i, j] = mixture.mu[i, j] # scalar

    K = mixture.var.shape[0]
    vars = np.empty((K,))
    size = delta.sum(axis = 1) # (K, )
    for (i, mu) in enumerate(mus):
        diff = delta * X - delta * mu # (n, d)
        distSq = np.sum(diff**2, axis = 1) # (n, )
        vars[i] = np.dot(post[:, i], distSq) / np.dot(post[:, i], size) # scalar
        if vars[i] < min_variance:
            vars[i] = min_variance

    ps = post.mean(axis = 0)

    return GaussianMixture(mus, vars, ps)


def run(X: np.ndarray, mixture: GaussianMixture,
        post: np.ndarray) -> Tuple[GaussianMixture, np.ndarray, float]:
    """Runs the mixture model

    Args:
        X: (n, d) array holding the data
        post: (n, K) array holding the soft counts
            for all components for all examples

    Returns:
        GaussianMixture: the new gaussian mixture
        np.ndarray: (n, K) array holding the soft counts
            for all components for all examples
        float: log-likelihood of the current assignment
    """
    post, oldLL = estep(X, mixture)
    while(True):
        mixture = mstep(X, post, mixture)
        post, newLL = estep(X, mixture)
        if newLL - oldLL <= - 1e-6 * newLL:
            return mixture, post, newLL
        else:
            oldLL = newLL


def fill_matrix(X: np.ndarray, mixture: GaussianMixture) -> np.ndarray:
    """Fills an incomplete matrix according to a mixture model

    Args:
        X: (n, d) array of incomplete data (incomplete entries =0)
        mixture: a mixture of gaussians

    Returns
        np.ndarray: a (n, d) array with completed data
    """
    n = X.shape[0]
    K = mixture.p.shape[0]

    f = np.empty((n,K))
    # the entry (i,j) is the value
    # f(i, j) = log (prob of cluster j) + log(prob density of point X[i] being generated by cluster j)
    for (i,x) in enumerate(X):
        # find the support of point x and the clean_x
        supp = x != 0
        dim = supp.sum()
        clean_x = x[supp]

        # Compute the pdf for each cluster
        # clean_mus = mixture.mu[:,supp] # K rows, each one is the mean of a cluster
        # diff = clean_x - clean_mus # K rows, each one is x - mu
        # normSq = np.sum(diff**2, axis = 1) # K rows, each one is the sum of squares
        # var = mixture.var # K rows, each one is the variance
        # logpdf = - normSq / (2 * var) - dim * np.log(2 * np.pi * var) / 2 # Calculation in each row
        # f[i] = logpdf

        for j in range(K):
            mu = mixture.mu[j][supp]
            var = mixture.var[j]
            logpdf = - np.sum((clean_x - mu)**2)/ (2*var) - dim * np.log(2*np.pi*var) / 2
            f[i, j] = logpdf

    f = f + np.log(mixture.p + 1e-16)

    point_log_likelihood = np.apply_along_axis(logsumexp, 1 , f)

    log_posterior = f - point_log_likelihood[:, None]

    soft_counts = np.exp(log_posterior)

    mu = mixture.mu # (K, d)
    prediction = soft_counts @ mu # (n, d)

    holes = X == 0 # (n, d)
    final = X + holes * prediction
    return final
